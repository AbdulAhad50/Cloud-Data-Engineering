AKIA6ANVOGWXUYDVUXF5
sAYzdq/uahLUbIqH5iVkDXMjGupCxJDqBtr2HJ3g



arn:aws:iam::962990060975:role/service-role/AWSGlueServiceRole-s3

-- Step:1:
-- Create a snowflake account for 30 days.
-- Set the role to ACCOUNTADMIN


-- Step:2

create or replace warehouse Athena_Warehouse_data;
create or replace database athena_s3_data;
create or replace schema athena_s3.ATHENA_LOAD_DATA;



-- external stage creation

CREATE OR REPLACE STAGE ATHENA_DATA_LOAD
URL = "s3://athena-query-results-s3-etl/data/china/341c796c-80f9-4f6e-9232-072640185258.csv"
CREDENTIALS = (AWS_KEY_ID='AKIA6ANVOGWXUYDVUXF5', AWS_SECRET_KEY='sAYzdq/uahLUbIqH5iVkDXMjGupCxJDqBtr2HJ3g');

-- list stage
LIST @ATHENA_DATA_LOAD;


CREATE OR REPLACE TABLE CHINA_DATA (
  "Index" VARCHAR,
  "Customer Id" VARCHAR,
  "First Name" VARCHAR,
  "Last Name" VARCHAR,
  "Company" VARCHAR,
  "City" VARCHAR,
  "Country" VARCHAR,	
  "Phone 1" VARCHAR,
  "Phone 2" VARCHAR,
  "Email"	VARCHAR,
  "Subscription" VARCHAR,
  "Date" VARCHAR,
  "Website" VARCHAR
);

-- copy data from stage to table
COPY INTO CHINA_DATA FROM @ATHENA_DATA_LOAD
file_format = (TYPE = 'CSV', FIELD_DELIMITER=',', SKIP_HEADER=1) on_error = 'skip_file';

-- data should be there
SELECT * FROM CHINA_DATA;





-------------------------------------------------------------------------------------------------------







------------------------
-- Storage Integration
------------------------

-- giving privileges
USE ROLE ACCOUNTADMIN;
GRANT CREATE INTEGRATION ON ACCOUNT TO SYSADMIN;
USE ROLE SYSADMIN;

-- storage integration
CREATE OR REPLACE STORAGE INTEGRATION S3_INTEGRATION
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = '----------role arn --------------'
  ENABLED = TRUE
  STORAGE_ALLOWED_LOCATIONS = ('s3://snowflake-class-project');

-- giving privileges
USE ROLE ACCOUNTADMIN;
GRANT USAGE ON INTEGRATION S3_INTEGRATION TO ROLE SYSADMIN;
USE ROLE SYSADMIN;

-- valdating integration
DESC INTEGRATION S3_INTEGRATION;

-- Grant SYSADMIN usage and create privileges on the database and schema
USE ROLE ACCOUNTADMIN;
GRANT USAGE ON DATABASE PROJECT_DB TO ROLE SYSADMIN;
GRANT USAGE ON SCHEMA PROJECT_DB.PUBLIC TO ROLE SYSADMIN;
GRANT CREATE STAGE ON SCHEMA PROJECT_DB.PUBLIC TO ROLE SYSADMIN;

-- Switch back to SYSADMIN
USE ROLE SYSADMIN;


-- creating stage
CREATE OR REPLACE STAGE S3_INTEGRATEION_BULK_COPY_TESLA_STOCKS
  STORAGE_INTEGRATION = S3_INTEGRATION
  URL = 's3://snowflake-class-project/input-data/TSLA.csv'
  FILE_FORMAT = (TYPE = 'CSV', FIELD_DELIMITER=',', SKIP_HEADER=1);

-- validating integration
LIST @S3_INTEGRATEION_BULK_COPY_TESLA_STOCKS;

-- Need to give the snowflake ARN & ID

-- Making sure the table is empty
TRUNCATE TABLE TESLA_STOCKS;
SELECT * FROM TESLA_STOCKS;

-- Copy data using integration
COPY INTO TESLA_STOCKS FROM @S3_INTEGRATEION_BULK_COPY_TESLA_STOCKS;

-- data should be there
SELECT * FROM TESLA_STOCKS;

--==============================
-- Loading data using Snow Pipe
--===============================

-- 1. Stage the data
-- 2. Test the copy command
-- 3. Create pipe
-- 4. Configure cloud event / call snow pipe rest API

-- truncating data again
TRUNCATE TABLE TESLA_STOCKS;

-- dropping previously create integration & stage
DROP STORAGE INTEGRATION S3_INTEGRATION;
DROP STAGE S3_INTEGRATEION_BULK_COPY_TESLA_STOCKS;

-- HELP: https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration
-- Step 1: Configure access permissions (policy) for the S3 bucket
-- Step 2: Create the IAM Role in AWS and attach above policy you created.

-- Step 3: Create a Cloud Storage Integration in Snowflake
CREATE OR REPLACE STORAGE INTEGRATION S3_TESLA_INTEGRATION
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = '-------role arn-----------'
  ENABLED = TRUE
  STORAGE_ALLOWED_LOCATIONS = ('s3://snowflake-class-project');
  
-- Step 4: Retrieve the AWS IAM User for your Snowflake Account
DESC INTEGRATION S3_TESLA_INTEGRATION;

-- Step 5: Grant the IAM User Permissions to Access Bucket Objects
-- STORAGE_AWS_ROLE_ARN 
-- STORAGE_AWS_EXTERNAL_ID

-- Step 6: Create file format for external stage
CREATE OR REPLACE FILE FORMAT S3_TESLA_STAGE_FORMAT
    TYPE= 'CSV'
    FIELD_DELIMITER=','
    SKIP_HEADER=1;

drop 
    
-- Step 6: Create an external stage using file format createbavove
CREATE or replace STAGE S3_TESLA_STAGE
  STORAGE_INTEGRATION = S3_TESLA_INTEGRATION
  URL = 's3://snowflake-class-project/input-data/'
  FILE_FORMAT = S3_TESLA_STAGE_FORMAT;


-- Step 7: Create a COPY Into Command
-- HELP: https://docs.snowflake.com/en/user-guide/data-load-s3-copy

COPY INTO TESLA_STOCKS FROM @S3_TESLA_STAGE;

-- validating & dropping again for pip
SELECT * FROM TESLA_STOCKS;
TRUNCATE TABLE TESLA_STOCKS;

--  Creating Pipe 
CREATE OR REPLACE PIPE S3_TESLA_PIPE --name
AUTO_INGEST=TRUE
AS
COPY INTO TESLA_STOCKS FROM @S3_TESLA_STAGE;

-- Configure cloud event / call snow pipe rest API (S3_TESLA_EVENT_NOTICTATION)
SHOW PIPES;

-- Data should be there auotmatically
SELECT * FROM TESLA_STOCKS;

-- DROPPING PIPE
DROP PIPE S3_TESLA_PIPE;


----------------
-- TIME TRAVEL
----------------
SELECT * FROM TESLA_STOCKS order by DATE desc;

-- dropping & getting back the table (time travel)
DROP TABLE TESLA_STOCKS;
UNDROP TABLE TESLA_STOCKS;

-- updating values
UPDATE TESLA_STOCKS SET OPEN_VALUE=200 WHERE DATE = '2022-08-01';

-- getting data beofre last upodate query
SELECT * FROM TESLA_STOCKS BEFORE (statement => '01bf337e-0000-3406-005a-1e0b0003fe22') ORDER BY DATE DESC;