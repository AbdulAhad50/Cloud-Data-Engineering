# Real-Time Stock Market Data Pipeline with Apache Kafka & AWS ‚òÅÔ∏èüöÄ

A complete end-to-end real-time data streaming pipeline that ingests stock market data, streams it via Kafka (running on EC2), lands it in S3 in real-time, catalogs it with Glue Crawler, and enables live querying using Athena ‚Äî proving true real-time data orchestration.

### Architecture
```
Dataset (CSV)
     ‚Üì
Python Producer (Local/Jupyter)
     ‚Üì
Apache Kafka (on EC2 t2.micro)
     ‚Üì
Python Consumer (Local/Jupyter)
     ‚Üì
Amazon S3 Bucket (raw JSON files every second)
     ‚Üì
AWS Glue Crawler ‚Üí Glue Catalog Table
     ‚Üì
Amazon Athena ‚Üí Live SQL queries (row count increases every second üî•)
```

### Architecture Diagram

![Architecture Diagram](./architecture_diagram.png)

### What This Project Proves
- Kafka producer/consumer running in real Python scripts
- Kafka cluster accessible from local machine (public IP + security group magic)
- Real-time data dumping into S3 as individual JSON files
- Glue Crawler automatically updating the catalog as new files arrive
- Athena showing live row count increase ‚Üí actual real-time pipeline working

### Tech Stack
- Apache Kafka 3.9.1 (self-hosted on EC2)
- Python (kafka-python, pandas, boto3)
- AWS Services: EC2, S3, Glue, Athena, IAM
- Jupyter Notebook for producer & consumer

### Setup & Run (Step-by-Step)

1. Launch EC2 (t2.micro, 16GB storage recommended)  
   ‚Üí Open port 9092 in security group (your IP or 0.0.0.0/0 for testing)

2. Install Kafka + Java on EC2
```bash
wget https://downloads.apache.org/kafka/3.9.1/kafka_2.13-3.9.1.tgz
tar -xvf kafka_2.13-3.9.1.tgz
sudo yum install java-11-amazon-corretto-devel -y   # I used java-25 in video but 11 also works
cd kafka_2.13-3.9.1
```

3. Edit server.properties (important for external access)
```properties
advertised.listeners=PLAINTEXT://YOUR_EC2_PUBLIC_IP:9092
```
Remove the `#` and replace with actual public IP.

4. Start Zookeeper & Kafka (2 separate SSH terminals)
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties
```

5. Create topic
```bash
bin/kafka-topics.sh --create --topic demo_testing2 --bootstrap-server YOUR_EC2_IP:9092 --replication-factor 1 --partitions 1
```

6. Local machine setup
```bash
pip install kafka-python pandas boto3
aws configure   # put your access key & secret
```

7. Run Producer (`Kafka_Producer.ipynb`)
‚Üí Samples random rows from `indexProcessed.csv` and sends to Kafka every 1 second (real-time simulation)

8. Run Consumer (`Kafka_Consumer.ipynb`)
‚Üí Consumes from Kafka and writes each message as `stock_market_{count}.json` in S3 bucket

9. Create S3 bucket ‚Üí Glue Crawler ‚Üí IAM role with S3 + Glue + Athena permissions  
   Run crawler ‚Üí Table appears in Glue Catalog

10. Open Athena ‚Üí Query:
```sql
SELECT COUNT(*) FROM "kafka_stock_db"."kafka_real_time_data_pipeline";
```
Refresh every few seconds ‚Üí row count keeps increasing = REAL-TIME MAGIC ACHIEVED ‚ú®

### Proof (Live Demo Feel)
Every second a new JSON file lands in S3 ‚Üí Crawler updates catalog ‚Üí Athena shows +rows ‚Üí You just built a real-time data lake.

### Repo Contents
- `Kafka_Producer.ipynb` ‚Üí Real-time producer
- `Kafka_Consumer.ipynb` ‚Üí Consumer ‚Üí S3
- `indexProcessed.csv` ‚Üí Sample stock dataset (104,761 rows)
- `commands.txt` ‚Üí All terminal commands used
